---
name: test-driven-development
displayName: "æµ‹è¯•é©±åŠ¨å¼€å‘"
version: "2.1.0"
description: Use when implementing any feature or bugfix, before writing implementation code

triggers:
  keywords:
    # ===== ç›´æ¥è§¦å‘ TDD =====
    - "TDD"
    - "æµ‹è¯•é©±åŠ¨"
    - "å…ˆå†™æµ‹è¯•"
    - "ç”¨TDD"
    - "ä½¿ç”¨TDD"

    # ===== è¦†ç›–ç‡ç›¸å…³ï¼ˆæ‰‹åŠ¨æ£€æŸ¥ï¼‰=====
    - "è¦†ç›–ç‡"
    - "æµ‹è¯•è¦†ç›–ç‡"
    - "ä»£ç è¦†ç›–ç‡"
    - "è¦†ç›–ç‡æ£€æŸ¥"
    - "æ£€æŸ¥è¦†ç›–ç‡"
    - "æµ‹è¯•è¦†ç›–"
    - "coverage"

    # ===== æ˜ç¡®è¦æ±‚æµ‹è¯• =====
    - "å†™æµ‹è¯•"
    - "å†™æµ‹è¯•ç”¨ä¾‹"
    - "ç¼–å†™æµ‹è¯•"
    - "éœ€è¦æµ‹è¯•"
    - "è¦å†™æµ‹è¯•"
    - "åŠ ä¸Šæµ‹è¯•"
    - "è¡¥å……æµ‹è¯•"

    # ===== æµ‹è¯•ç±»å‹ =====
    - "å•å…ƒæµ‹è¯•"
    - "é›†æˆæµ‹è¯•"
    - "æµ‹è¯•ç”¨ä¾‹"

    # ===== è´¨é‡è¦æ±‚ =====
    - "æµ‹è¯•è´¨é‡"
    - "é«˜è´¨é‡ä»£ç "
    - "ä»£ç è´¨é‡"

  auto_trigger: false              # âœ… å®Œå…¨æ‰‹åŠ¨è§¦å‘ï¼Œéœ€è¦ç”¨æˆ·æ˜ç¡®è¦æ±‚
  confidence_threshold: 0.7       # è§¦å‘ç½®ä¿¡åº¦é˜ˆå€¼

tools:
  required:
    - Write
  optional:
    - Bash
    - Read

permissions:
  level: "write"
  scope:
    - "file:read"
    - "file:write"

context:
  mode: inline
  isolation: false

hot_reload: true
progressive_load: true

metadata:
  category: "testing"
  tags:
    - "TDD"
    - "æµ‹è¯•"
    - "å¼€å‘æµç¨‹"
  author: "Smart Flow Team"
  license: "MIT"
  updated_at: "2026-01-12"

scope:
  level: "project"
  priority: 75
---

# Test-Driven Development (TDD)

## Overview

Write the test first. Watch it fail. Write minimal code to pass.

**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.

**Violating the letter of the rules is violating the spirit of the rules.**

## How to Trigger This Skill

This skill uses **MANUAL TRIGGER** mode. You must explicitly request it using one of these keywords:

### ğŸ¯ Direct TDD Requests
```
"ç”¨ TDD å¼€å‘ç”¨æˆ·ç™»å½•åŠŸèƒ½"
"æµ‹è¯•é©±åŠ¨å¼€å‘å®ç°è®¢å•æ¥å£"
"å…ˆå†™æµ‹è¯•ï¼Œåå†™ä»£ç "
```

### ğŸ“Š Coverage Check Requests
```
"æ£€æŸ¥è¦†ç›–ç‡"
"æµ‹è¯•è¦†ç›–ç‡è¾¾æ ‡äº†å—ï¼Ÿ"
"è¿è¡Œè¦†ç›–ç‡æ£€æŸ¥"
"æŸ¥çœ‹ä»£ç è¦†ç›–ç‡"
```

### âœï¸ Test Writing Requests
```
"å†™æµ‹è¯•ç”¨ä¾‹"
"ç¼–å†™å•å…ƒæµ‹è¯•"
"ç»™è¿™ä¸ªåŠŸèƒ½åŠ ä¸Šæµ‹è¯•"
"è¡¥å……æµ‹è¯•ç”¨ä¾‹"
```

### ğŸ† Quality Requests
```
"ç¡®ä¿ä»£ç è´¨é‡"
"éœ€è¦é«˜è´¨é‡çš„æµ‹è¯•"
"æµ‹è¯•è´¨é‡è¦æ±‚"
```

**What Happens When Triggered:**
1. âœ… Enforce TDD workflow (Red-Green-Refactor)
2. âœ… Require 95% coverage for core modules (strategy/, risk/, api/)
3. âœ… Require 80% coverage for general modules (utils/, infra/, scripts/)
4. âœ… Update TEST_PROGRESS.md automatically
5. âœ… Block completion if coverage not met

**Important**: This skill will NOT auto-trigger. You must explicitly request testing or TDD.

## Project-Specific Standards

### ğŸ¯ Quantitative Trading Project Rules

This TDD skill enforces **STRICT** testing standards for quantitative trading projects:

#### 1. Dual Coverage Standards (åŒæ ‡)

| Module Tier | Coverage Threshold | Example Directories | Requirements |
|-------------|-------------------|---------------------|--------------|
| **Core (Tier 1)** | **95%** | `src/strategy/`, `src/risk/`, `src/api/` | Must cover ALL if/else branches + exception handling |
| **General (Tier 2)** | **80%** | `src/utils/`, `src/infra/`, `scripts/` | Core logic flows must be 100% covered |

**RED LINE**: AI MUST NOT mark task complete if coverage below threshold.

**Verification Command**:
```bash
npm run test:cov          # Jest/Vitest projects
pytest --cov=src          # Python projects
```

#### 2. Test Progress Tracking

**MANDATORY**: Maintain `TEST_PROGRESS.md` in project root.

Update after EVERY feature completion:

**Status Markers**:
- âœ… = Tested and passed
- â³ = Partially tested (edge cases pending)
- âŒ = Failed or not tested

**Required Fields**:
- Module name
- Sub-feature/API
- Test status (âœ…/â³/âŒ)
- Actual coverage percentage
- Test file path
- Notes (e.g., "Mock data construction needed")

**Example Entry**:
```markdown
### é£é™©æ§åˆ¶ (Risk Control)
- [âœ…] æ­¢æŸé€»è¾‘è§¦å‘ (98%) - `tests/risk/stop-loss.test.ts`
- [â³] æç«¯è¡Œæƒ…ä¸‹çš„æ»‘ç‚¹æ¨¡æ‹Ÿ (NA) - åŸå› ï¼šMockæ•°æ®æ„é€ ä¸­
```

#### 3. Task Closure Requirements

Before ending ANY task:

- [ ] Update `TEST_PROGRESS.md`
- [ ] Run coverage check: `npm run test:cov` or `pytest --cov`
- [ ] Verify API docs sync with code
- [ ] Report current core module coverage in response
- [ ] Fix any failing tests

**Exception**: Only skip with human partner's explicit permission.

#### 4. Documentation Splitting Rule

**MAX 1000 lines per document**.

When approaching limit:
- Split by feature (e.g., `API_AUTH.md`, `API_TRADE.md`)
- Update index with links to sub-docs

## When to Use

**Always:**
- New features
- Bug fixes
- Refactoring
- Behavior changes

**Exceptions (ask your human partner):**
- Throwaway prototypes
- Generated code
- Configuration files

Thinking "skip TDD just this once"? Stop. That's rationalization.

## The Iron Law

```
NO PRODUCTION CODE WITHOUT A FAILING TEST FIRST
```

Write code before the test? Delete it. Start over.

**No exceptions:**
- Don't keep it as "reference"
- Don't "adapt" it while writing tests
- Don't look at it
- Delete means delete

Implement fresh from tests. Period.

## Red-Green-Refactor

```dot
digraph tdd_cycle {
    rankdir=LR;
    red [label="RED\nWrite failing test", shape=box, style=filled, fillcolor="#ffcccc"];
    verify_red [label="Verify fails\ncorrectly", shape=diamond];
    green [label="GREEN\nMinimal code", shape=box, style=filled, fillcolor="#ccffcc"];
    verify_green [label="Verify passes\nAll green", shape=diamond];
    refactor [label="REFACTOR\nClean up", shape=box, style=filled, fillcolor="#ccccff"];
    next [label="Next", shape=ellipse];

    red -> verify_red;
    verify_red -> green [label="yes"];
    verify_red -> red [label="wrong\nfailure"];
    green -> verify_green;
    verify_green -> refactor [label="yes"];
    verify_green -> green [label="no"];
    refactor -> verify_green [label="stay\ngreen"];
    verify_green -> next;
    next -> red;
}
```

### RED - Write Failing Test

Write one minimal test showing what should happen.

<Good>
```typescript
test('retries failed operations 3 times', async () => {
  let attempts = 0;
  const operation = () => {
    attempts++;
    if (attempts < 3) throw new Error('fail');
    return 'success';
  };

  const result = await retryOperation(operation);

  expect(result).toBe('success');
  expect(attempts).toBe(3);
});
```
Clear name, tests real behavior, one thing
</Good>

<Bad>
```typescript
test('retry works', async () => {
  const mock = jest.fn()
    .mockRejectedValueOnce(new Error())
    .mockRejectedValueOnce(new Error())
    .mockResolvedValueOnce('success');
  await retryOperation(mock);
  expect(mock).toHaveBeenCalledTimes(3);
});
```
Vague name, tests mock not code
</Bad>

**Requirements:**
- One behavior
- Clear name
- Real code (no mocks unless unavoidable)

### Verify RED - Watch It Fail

**MANDATORY. Never skip.**

```bash
npm test path/to/test.test.ts
```

Confirm:
- Test fails (not errors)
- Failure message is expected
- Fails because feature missing (not typos)

**Test passes?** You're testing existing behavior. Fix test.

**Test errors?** Fix error, re-run until it fails correctly.

### GREEN - Minimal Code

Write simplest code to pass the test.

<Good>
```typescript
async function retryOperation<T>(fn: () => Promise<T>): Promise<T> {
  for (let i = 0; i < 3; i++) {
    try {
      return await fn();
    } catch (e) {
      if (i === 2) throw e;
    }
  }
  throw new Error('unreachable');
}
```
Just enough to pass
</Good>

<Bad>
```typescript
async function retryOperation<T>(
  fn: () => Promise<T>,
  options?: {
    maxRetries?: number;
    backoff?: 'linear' | 'exponential';
    onRetry?: (attempt: number) => void;
  }
): Promise<T> {
  // YAGNI
}
```
Over-engineered
</Bad>

Don't add features, refactor other code, or "improve" beyond the test.

### Verify GREEN - Watch It Pass

**MANDATORY.**

```bash
npm test path/to/test.test.ts
```

Confirm:
- Test passes
- Other tests still pass
- Output pristine (no errors, warnings)

**Test fails?** Fix code, not test.

**Other tests fail?** Fix now.

### REFACTOR - Clean Up

After green only:
- Remove duplication
- Improve names
- Extract helpers

Keep tests green. Don't add behavior.

### Repeat

Next failing test for next feature.

## Good Tests

| Quality | Good | Bad |
|---------|------|-----|
| **Minimal** | One thing. "and" in name? Split it. | `test('validates email and domain and whitespace')` |
| **Clear** | Name describes behavior | `test('test1')` |
| **Shows intent** | Demonstrates desired API | Obscures what code should do |

## Why Order Matters

**"I'll write tests after to verify it works"**

Tests written after code pass immediately. Passing immediately proves nothing:
- Might test wrong thing
- Might test implementation, not behavior
- Might miss edge cases you forgot
- You never saw it catch the bug

Test-first forces you to see the test fail, proving it actually tests something.

**"I already manually tested all the edge cases"**

Manual testing is ad-hoc. You think you tested everything but:
- No record of what you tested
- Can't re-run when code changes
- Easy to forget cases under pressure
- "It worked when I tried it" â‰  comprehensive

Automated tests are systematic. They run the same way every time.

**"Deleting X hours of work is wasteful"**

Sunk cost fallacy. The time is already gone. Your choice now:
- Delete and rewrite with TDD (X more hours, high confidence)
- Keep it and add tests after (30 min, low confidence, likely bugs)

The "waste" is keeping code you can't trust. Working code without real tests is technical debt.

**"TDD is dogmatic, being pragmatic means adapting"**

TDD IS pragmatic:
- Finds bugs before commit (faster than debugging after)
- Prevents regressions (tests catch breaks immediately)
- Documents behavior (tests show how to use code)
- Enables refactoring (change freely, tests catch breaks)

"Pragmatic" shortcuts = debugging in production = slower.

**"Tests after achieve the same goals - it's spirit not ritual"**

No. Tests-after answer "What does this do?" Tests-first answer "What should this do?"

Tests-after are biased by your implementation. You test what you built, not what's required. You verify remembered edge cases, not discovered ones.

Tests-first force edge case discovery before implementing. Tests-after verify you remembered everything (you didn't).

30 minutes of tests after â‰  TDD. You get coverage, lose proof tests work.

## Common Rationalizations

| Excuse | Reality |
|--------|---------|
| "Too simple to test" | Simple code breaks. Test takes 30 seconds. |
| "I'll test after" | Tests passing immediately prove nothing. |
| "Tests after achieve same goals" | Tests-after = "what does this do?" Tests-first = "what should this do?" |
| "Already manually tested" | Ad-hoc â‰  systematic. No record, can't re-run. |
| "Deleting X hours is wasteful" | Sunk cost fallacy. Keeping unverified code is technical debt. |
| "Keep as reference, write tests first" | You'll adapt it. That's testing after. Delete means delete. |
| "Need to explore first" | Fine. Throw away exploration, start with TDD. |
| "Test hard = design unclear" | Listen to test. Hard to test = hard to use. |
| "TDD will slow me down" | TDD faster than debugging. Pragmatic = test-first. |
| "Manual test faster" | Manual doesn't prove edge cases. You'll re-test every change. |
| "Existing code has no tests" | You're improving it. Add tests for existing code. |

## Red Flags - STOP and Start Over

- Code before test
- Test after implementation
- Test passes immediately
- Can't explain why test failed
- Tests added "later"
- Rationalizing "just this once"
- "I already manually tested it"
- "Tests after achieve the same purpose"
- "It's about spirit not ritual"
- "Keep as reference" or "adapt existing code"
- "Already spent X hours, deleting is wasteful"
- "TDD is dogmatic, I'm being pragmatic"
- "This is different because..."

**All of these mean: Delete code. Start over with TDD.**

## Example: Bug Fix

**Bug:** Empty email accepted

**RED**
```typescript
test('rejects empty email', async () => {
  const result = await submitForm({ email: '' });
  expect(result.error).toBe('Email required');
});
```

**Verify RED**
```bash
$ npm test
FAIL: expected 'Email required', got undefined
```

**GREEN**
```typescript
function submitForm(data: FormData) {
  if (!data.email?.trim()) {
    return { error: 'Email required' };
  }
  // ...
}
```

**Verify GREEN**
```bash
$ npm test
PASS
```

**REFACTOR**
Extract validation for multiple fields if needed.

## Coverage Verification

### Before Marking Complete

**MANDATORY CHECKS**:

1. **Run Coverage Report**:
   ```bash
   npm run test:cov          # For TypeScript/JavaScript
   pytest --cov=src --cov-report=term-missing  # For Python
   ```

2. **Verify Thresholds**:
   - Core modules (Tier 1) â‰¥ 95%
   - General modules (Tier 2) â‰¥ 80%

3. **Check Specific Files**:
   ```bash
   # Check specific file coverage
   npm run test:cov -- src/strategy/execution.ts
   ```

4. **View Uncovered Lines**:
   ```bash
   # Show missing line numbers
   npm run test:cov -- --coverage --verbose
   ```

**If Below Threshold**:
- âŒ DO NOT mark task complete
- âœ… Add tests for uncovered branches
- âœ… Re-run untilè¾¾æ ‡

### Coverage Report Template

After running tests, report format:

```
=== Coverage Report ===
Core Modules (Tier 1):
  src/strategy/execution.ts:  97% âœ…
  src/risk/stop-loss.ts:      94% âš ï¸  (Need +1%)
  src/api/order.ts:           96% âœ…

General Modules (Tier 2):
  src/utils/date.ts:          85% âœ…
  src/infra/database.ts:      78% âš ï¸  (Need +2%)

Overall: 92% (Target: 95% for core, 80% for general)
Status: â³ Pending improvements in risk/stop-loss.ts
```

## Verification Checklist

Before marking work complete:

- [ ] Every new function/method has a test
- [ ] Watched each test fail before implementing
- [ ] Each test failed for expected reason (feature missing, not typo)
- [ ] Wrote minimal code to pass each test
- [ ] All tests pass
- [ ] Output pristine (no errors, warnings)
- [ ] Tests use real code (mocks only if unavoidable)
- [ ] Edge cases and errors covered
- [ ] **Coverage meets threshold**: Core â‰¥95%, General â‰¥80%
- [ ] **TEST_PROGRESS.md updated** with current coverage
- [ ] **API docs synced** with implementation

Can't check all boxes? You skipped TDD. Start over.

## When Stuck

| Problem | Solution |
|---------|----------|
| Don't know how to test | Write wished-for API. Write assertion first. Ask your human partner. |
| Test too complicated | Design too complicated. Simplify interface. |
| Must mock everything | Code too coupled. Use dependency injection. |
| Test setup huge | Extract helpers. Still complex? Simplify design. |

## Debugging Integration

Bug found? Write failing test reproducing it. Follow TDD cycle. Test proves fix and prevents regression.

Never fix bugs without a test.

## Testing Anti-Patterns

When adding mocks or test utilities, read @testing-anti-patterns.md to avoid common pitfalls:
- Testing mock behavior instead of real behavior
- Adding test-only methods to production classes
- Mocking without understanding dependencies

## Final Rule

```
Production code â†’ test exists and failed first
Otherwise â†’ not TDD
```

No exceptions without your human partner's permission.

---

## Test Progress Documentation

### TEST_PROGRESS.md Template

Create `TEST_PROGRESS.md` in project root:

```markdown
# æµ‹è¯•è¿›åº¦è·Ÿè¸ª (Test Progress Tracking)

> **æ›´æ–°æ—¶é—´**: 2026-01-21
> **è¦†ç›–ç‡ç›®æ ‡**: æ ¸å¿ƒæ¨¡å— 95% | é€šç”¨æ¨¡å— 80%

---

## ğŸ“Š æ•´ä½“è¦†ç›–ç‡æ¦‚è§ˆ

| æ¨¡å—ç±»å‹ | å½“å‰è¦†ç›–ç‡ | ç›®æ ‡è¦†ç›–ç‡ | çŠ¶æ€ |
|---------|-----------|-----------|------|
| æ ¸å¿ƒæ¨¡å— (Tier 1) | -- | 95% | â³ |
| é€šç”¨æ¨¡å— (Tier 2) | -- | 80% | â³ |
| **æ•´ä½“é¡¹ç›®** | -- | 90% | â³ |

---

## ğŸ¯ æ ¸å¿ƒæ¨¡å— (Tier 1) - ç›®æ ‡: 95%

### ç­–ç•¥æ‰§è¡Œ (Strategy Execution)

| åŠŸèƒ½ | æµ‹è¯•çŠ¶æ€ | è¦†ç›–ç‡ | æµ‹è¯•æ–‡ä»¶ | å¤‡æ³¨ |
|------|---------|--------|---------|------|
| è®¢å•ä¸‹å•æ¥å£ | âœ… å·²å®Œæˆ | -- | `tests/order.test.ts` | æ¶µç›–å¸‚ä»·/é™ä»·å• |
| è®¢å•æ’¤å•é€»è¾‘ | â³ å¾…æµ‹è¯• | -- | - | éœ€å¤„ç†ç½‘ç»œå»¶è¿Ÿ |
| è®¢å•çŠ¶æ€æŸ¥è¯¢ | âŒ æœªæµ‹è¯• | -- | - | - |

**å½“å‰æ¨¡å—è¦†ç›–ç‡**: -- %

### é£é™©æ§åˆ¶ (Risk Control)

| åŠŸèƒ½ | æµ‹è¯•çŠ¶æ€ | è¦†ç›–ç‡ | æµ‹è¯•æ–‡ä»¶ | å¤‡æ³¨ |
|------|---------|--------|---------|------|
| æ­¢æŸé€»è¾‘è§¦å‘ | â³ éƒ¨åˆ†å®Œæˆ | -- | `tests/risk/stop-loss.test.ts` | è¾¹ç•Œæ¡ä»¶æœªè¦†ç›– |
| æŒä»“é™é¢æ£€æŸ¥ | âŒ æœªæµ‹è¯• | -- | - | - |
| æç«¯è¡Œæƒ…å¤„ç† | âŒ æœªæµ‹è¯• | -- | - | éœ€æ„é€ æ¨¡æ‹Ÿæ•°æ® |

**å½“å‰æ¨¡å—è¦†ç›–ç‡**: -- %

### API æ¥å£ (API Layer)

| åŠŸèƒ½ | æµ‹è¯•çŠ¶æ€ | è¦†ç›–ç‡ | æµ‹è¯•æ–‡ä»¶ | å¤‡æ³¨ |
|------|---------|--------|---------|------|
| ç”¨æˆ·è®¤è¯ | âŒ æœªæµ‹è¯• | -- | - | - |
| è®¢å•ç®¡ç† | âŒ æœªæµ‹è¯• | -- | - | - |

**å½“å‰æ¨¡å—è¦†ç›–ç‡**: -- %

---

## ğŸ”§ é€šç”¨æ¨¡å— (Tier 2) - ç›®æ ‡: 80%

### æ•°æ®å¤„ç† (Data Processing)

| åŠŸèƒ½ | æµ‹è¯•çŠ¶æ€ | è¦†ç›–ç‡ | æµ‹è¯•æ–‡ä»¶ | å¤‡æ³¨ |
|------|---------|--------|---------|------|
| Kçº¿æ•°æ®æ¸…æ´— | âŒ æœªæµ‹è¯• | -- | - | éœ€å¤„ç†ç©ºå€¼ |
| æ•°æ®æ ¼å¼è½¬æ¢ | âŒ æœªæµ‹è¯• | -- | - | - |

**å½“å‰æ¨¡å—è¦†ç›–ç‡**: -- %

### åŸºç¡€è®¾æ–½ (Infrastructure)

| åŠŸèƒ½ | æµ‹è¯•çŠ¶æ€ | è¦†ç›–ç‡ | æµ‹è¯•æ–‡ä»¶ | å¤‡æ³¨ |
|------|---------|--------|---------|------|
| æ•°æ®åº“è¿æ¥ | âŒ æœªæµ‹è¯• | -- | - | - |
| æ—¥å¿—ç³»ç»Ÿ | âŒ æœªæµ‹è¯• | -- | - | - |

**å½“å‰æ¨¡å—è¦†ç›–ç‡**: -- %

---

## ğŸš¨ å¾…ä¿®å¤é—®é¢˜

| ä¼˜å…ˆçº§ | é—®é¢˜æè¿° | å½±å“æ¨¡å— | è´Ÿè´£äºº | æˆªæ­¢æ—¥æœŸ |
|-------|---------|---------|--------|---------|
| P0 | æ­¢æŸé€»è¾‘è¾¹ç•Œæ¡ä»¶è¦†ç›–ä¸è¶³ | risk/stop-loss | - | - |

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **2026-01-21**: åˆ›å»ºæµ‹è¯•è¿›åº¦è·Ÿè¸ªæ–‡æ¡£

---

**å›¾ä¾‹è¯´æ˜**:
- âœ… = å·²å®Œæˆæµ‹è¯•å¹¶é€šè¿‡
- â³ = éƒ¨åˆ†å®Œæˆæˆ–è¿›è¡Œä¸­
- âŒ = æœªæµ‹è¯•æˆ–æµ‹è¯•å¤±è´¥
- -- = å¾…æµ‹é‡
```

### How to Use

1. **After Each Feature**:
   ```bash
   npm run test:cov
   # Copy coverage % to relevant section
   # Update test status (âœ…/â³/âŒ)
   ```

2. **Before Task Complete**:
   - Verify all Tier 1 modules â‰¥ 95%
   - Verify all Tier 2 modules â‰¥ 80%
   - Update "æ•´ä½“è¦†ç›–ç‡æ¦‚è§ˆ" table

3. **Weekly Review**:
   - Check "å¾…ä¿®å¤é—®é¢˜" section
   - Prioritize modules below threshold

### AI Auto-Update Command

When AI completes feature, auto-execute:

```bash
# Update TEST_PROGRESS.md
npm run test:cov | tee coverage_report.txt
# Parse and update markdown
```

---

**END OF SKILL DOCUMENTATION**
